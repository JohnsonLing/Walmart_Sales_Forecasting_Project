---
title: 'DSO 522: Final Project - Walmart Sales Prediction'
author: 'Group 8'
output:
  pdf_document: default
  word_document: default
header-includes: \usepackage{color}
editor_options: 
  chunk_output_type: console
---

- Library Import 
```{r}
library(ggplot2)
library(forecast)
library(zoo)
library(dplyr)
library(skimr)
```

## EDA
Exploring the dataset
```{r}
# Explore the original dataset
data = read.csv("Walmart.csv", header = T)
head(data,3)

# Explore the dataset
# 1. Dimension of the dataset
dim(data)

#2. summary of the dataset
summary(data)

#3. Skim summary of the dataset
skim(data)
```


## Data Preparation

Data Cleaning
```{r}
# Read Clean
data = read.csv("Walmart.csv", header = T)
head(data,3)

# Create Time Target (143 weeks, 45 stores in total)
data$week = rep(1:143,45)

# Install dplyr 
#install.packages("rlang")
#install.packages("dplyr")
#library(rlang)
#library(dplyr)
require(dplyr)

data_clean <- data%>%group_by(Date) %>%
  summarise(Weekly_Sales = mean(Weekly_Sales),
            week = mean(week),
            Holiday_Flag = mean(Holiday_Flag),
            Temperature = mean(Temperature),
            Fuel_Price = mean(Fuel_Price),
            CPI = mean(CPI), 
            Unemployment = mean(Unemployment),
            .groups = 'drop')

data_clean <- data_clean[order(data_clean$week),]
# View(data_clean)

# Data Weekly Average Calculation
# data_clean = aggregate(data, list(DateAgg = data$Date), mean)
# index.ord = order(data_clean$week)
# data_clean = data_clean[index.ord,]
# data_clean = data_clean[,c(1,4,5,6,7,8,9)]
# New Dataset: Only Include DataAgg, Weekly_Sales, Holiday_Flag, Temperature, Fuel_Price, CPI, Unemployment
head(data_clean,5)
```

Transforming to Time Series
```{r}
WeeklySales.ts = ts(data_clean$Weekly_Sales, start = c(2010,6), frequency = 52)
plot(WeeklySales.ts)
```

Train Test Split
training set: '2010-02-05' to '2012-02-03' (105 weeks)
validation set: '2012-02-03' to '2012-10-26' (38 weeks)

```{r}
n = length(WeeklySales.ts)
nValid = 38
nTrain = n - nValid

WeeklySales.train = window(WeeklySales.ts, 
                          start = c(2010,6),
                          end = c(2010,5+nTrain))
WeeklySales.test = window(WeeklySales.ts,
                          start = c(2010,5+nTrain+1))
WeeklySales.train
WeeklySales.test
```


## Naive & Seasonal Forecast
```{r}
# Naive Forecast
naive = naive(WeeklySales.train, h=length(WeeklySales.test))
accuracy(naive, WeeklySales.test)

# Seasonal Forecast
seasonal = snaive(WeeklySales.train, h= length(WeeklySales.test))
accuracy(seasonal, WeeklySales.test)

# Graph of Naive and seasonal forecast
autoplot(WeeklySales.train)+
  autolayer(naive, series ="Naive Forecast", PI=FALSE)+
  autolayer(seasonal, series="Seasonal Forecast", PI=FALSE)+
  autolayer(WeeklySales.test, series="Observed")

```
For Naive forecast, training set RMSE is 158487.6, and testing RMSE is 49706.2. 
For Seasonla Forecast, training set RMSE is 43443.57, and testing RMSE is 45729.70. 


## Exponential Smoothing & Moving Average/Trailing

## ARIMA/ Seasonal ARIMA
Step 1: De-trend the data 
```{r}
# Remove the trend from of the time series data
dWeeklySales.ts = diff(WeeklySales.ts,1)
autoplot(dWeeklySales.ts)

# Train Test Split
dn = length(dWeeklySales.ts)
dValid = 38
dTrain = dn - dValid

dWeeklySales.train = window(dWeeklySales.ts, 
                          start = c(2010,7),
                          end = c(2010,6+dTrain))
dWeeklySales.test = window(dWeeklySales.ts,
                          start = c(2010,6+dTrain+1))

```

Step 2: Use the training set and plot the ACF and PACF functions of the growth rate of the weekly sales found in previous part.
```{r}
par(mfrow = c(1,2))
Acf(dWeeklySales.train,52, main = "")
Pacf(dWeeklySales.train,52,main = "")
par(mfrow = c(1,1))
```

According to the ACF and PACF plots, the ACF is tailing of and PACF is also tailing off within the first season.The ACF is cutting off after the first season. 
We will try SARMA(1,0,1)x(0,0,1) as our baseline model

Step 3: Modeling
```{r}
# SARMA(1,0,1)x(0,0,1)
m1 = Arima(dWeeklySales.train,
           order = c(1,0,1),
           seasonal = list(order = c(0,0,1),period = 52))
m1.predict <- forecast(m1, h = dValid, level = 0)

autoplot(dWeeklySales.test, series = 'Actual')+
  autolayer(m1.predict, series = 'Predicted')

accuracy(m1.predict, dWeeklySales.test)
autoplot(m1.predict)
checkresiduals(m1)
```
For SARMA(1,0,1)x(0,0,1), the RMSE in training set is 76997.19 and RMSE in Test set is 50804.47.

```{r}
# SARMA(3,0,5)x(0,0,1)
m2 = Arima(dWeeklySales.train, 
           order = c(3,0,5),
           seasonal = list(order = c(0,0,1),period = 52))
m2.predict <- forecast(m2, h = dValid, level = 0)

autoplot(dWeeklySales.test, series = 'Actual')+
  autolayer(m2.predict, series = 'Predicted')

accuracy(m2.predict, dWeeklySales.test)

autoplot(m2.predict)
checkresiduals(m2.predict)
```
For SARMA(3,0,5)x(0,0,1), the RMSE in training set is 69168.37 and RMSE in Test set is 52014.26.

```{r}
# SARMA(1,0,4)x(0,0,1)
m3 = Arima(dWeeklySales.train, 
           order = c(1,0,4),
           seasonal = list(order = c(0,0,1),period = 52))
m3.predict <- forecast(m3, h = dValid, level = 0)

autoplot(dWeeklySales.test, series = 'Actual')+
  autolayer(m3.predict, series = 'Predicted')

accuracy(m3.predict, dWeeklySales.test)

autoplot(m3.predict)
checkresiduals(m3.predict)
```
For SARMA(1,0,4)x(0,0,1), the RMSE in training set is 73051.41 and RMSE in Test set is 50396.00. 


## Multiple Linear Regression

Step0: Add necessary columns
Add a column consider Blackfriday and Christmas
```{r}
data_clean$is_Xmas_BF = rep(0, 143)
for (week_flag in c(42,43,46,47))
{
  data_clean$is_Xmas_BF = ifelse(data_clean$is_Xmas_BF ==1,1,ifelse((data_clean$week-week_flag)/52==0|(data_clean$week-week_flag)/52==1,1,0))
}
```
Add a column consider the low point after Christmas.
```{r}
data_clean$is_NY = rep(0, 143)
for (week_flag in c(35,36,37,38,39,48,49,50,51))
{
  data_clean$is_NY = ifelse(data_clean$is_NY ==1,1,ifelse((data_clean$week-week_flag)/52==0|(data_clean$week-week_flag)/52==1,1,0))
}
```


Step1: Construct other factors
```{r}
HolidayFlag.ts = ts(data_clean$Holiday_Flag, start = c(2010,6), frequency = 52)
Temp.ts = ts(data_clean$Temperature, start = c(2010,6), frequency = 52)
plot(Temp.ts)
Fuel.ts = ts(data_clean$Fuel_Price, start = c(2010,6), frequency = 52)
plot(Fuel.ts)
Cpi.ts = ts(data_clean$CPI, start = c(2010,6), frequency = 52)
plot(Cpi.ts)
Rate.ts = ts(data_clean$Unemployment, start = c(2010,6), frequency = 52)
plot(Rate.ts)
Xmas.ts = ts(data_clean$is_Xmas_BF, start = c(2010,6), frequency = 52)
plot(Xmas.ts)
Ny.ts = ts(data_clean$is_NY, start = c(2010,6), frequency = 52)
plot(Ny.ts)
```

Step2: Find Relationships
```{r}
library(ggplot2)
# WeeklySales.ts = ts(log(data_clean$Weekly_Sales), start = c(2010,6), frequency = 52)
qplot(HolidayFlag.ts,WeeklySales.ts)
qplot(Temp.ts,WeeklySales.ts)
qplot(Fuel.ts,WeeklySales.ts)
qplot(Cpi.ts,WeeklySales.ts)
qplot(Rate.ts,WeeklySales.ts)
qplot(Xmas.ts,WeeklySales.ts)
qplot(Ny.ts, WeeklySales.ts)
```

Step3: Test CCF
```{r}
library(astsa)
Ccf(Temp.ts, WeeklySales.ts, 50)
# 20/42 lag should be significant
Ccf(Fuel.ts, WeeklySales.ts, 50)
# 31 lag should be significant
Ccf(Cpi.ts, WeeklySales.ts, 50)
Ccf(Rate.ts, WeeklySales.ts, 50)
Ccf(Xmas.ts, WeeklySales.ts, 50)
# really strong relationship in lag0
Ccf(Ny.ts, WeeklySales.ts, 50)

```

Step4: 
Single Model:
```{r}
library(forecast)
library(tidyverse)
# install.packages("rlang")
lag <- stats::lag
# test temperature
newdata1= ts.intersect(WeeklySales.ts, leadR20=lag(Temp.ts,-20), leadR42=lag(Temp.ts,-42))
# model 1
m1 = tslm(WeeklySales.ts~leadR20, data = newdata1)
summary(m1)
accuracy(m1)
# model 2
m2 = tslm(WeeklySales.ts~leadR42, data = newdata1)
summary(m2)
accuracy(m2)
# Obviously, both two models do not have good prediction power. However, first one is a little bit better.
```
```{r}
# test temperature
newdata2= ts.intersect(WeeklySales.ts, leadR31=lag(Fuel.ts,-31),Fuel.ts)
# model 3
m3 = tslm(WeeklySales.ts~leadR31, data = newdata2)
summary(m3)
accuracy(m3)
# model 4
m4 = tslm(WeeklySales.ts~Fuel.ts, data = newdata2)
summary(m4)
accuracy(m4)
# Obviously, both two models do not have good prediction power. However, first one is a little bit better.
```
Hybrid Model:
```{r}
# Build Final Model
newdata= ts.intersect(WeeklySales.ts, fuel=lag(Fuel.ts,-31), temp=lag(Temp.ts,-20), holiday = HolidayFlag.ts, cpi = Cpi.ts, rate = Rate.ts, xmas = Xmas.ts, ny= Ny.ts)
# model 5
m5 = tslm(WeeklySales.ts~ temp+fuel+xmas+ny, data = newdata)
summary(m5)
accuracy(m5)
# model 6
m6 = tslm(WeeklySales.ts~ temp+fuel+holiday+xmas+ny, data = newdata)
summary(m6)
accuracy(m6)
# model 7 
m7 = tslm(WeeklySales.ts~ temp+fuel+holiday+cpi+xmas+ny, data = newdata)
summary(m7)
accuracy(m7)
# model 8
m8 = tslm(WeeklySales.ts~ temp+fuel+holiday+cpi+rate+xmas+ny, data = newdata)
summary(m8)
accuracy(m8)
# All models are horrible, m7 should be the best.
```

```{r}
checkresiduals(m6)
acf2(resid(m6), 52)

# imply AR5

m6New = sarima(WeeklySales.ts, 5,0,0, xreg=cbind(Temp.ts,Fuel.ts, HolidayFlag.ts, Xmas.ts, Ny.ts))
# QUESNTION: HOW TO FORECAST?
```

Plot:
```{r}
autoplot(m6, ylab = "Walmart Sales", xlab = "Time",series = "Fitted Value", main = "Regression Prediction of Sales") + autolayer(WeeklySales.ts, series = "True Value")
```

Conclusion For Regression: Trash Variables and Trash Models!